package cse2520

import scala.util.Try

package object mapreduce {

  /**
   * Using this trait, mappers and reducers can select certain keys & values for
   * output by "emitting" them.
   *
   * How "emitting" works is usually hidden from the application - some production systems buffer data
   * only writing to the file system when some time / size condition is met. For the sake of simplicity,
   * we store the data in-memory until the actor explicitly writes it to the file system.
   */
  trait Emitter {
    /**
     * Selected key and value to output. This method mutates the state of given emitter.
     *
     * @param keyOut   key value
     * @param valueOut value
     */
    def emit(keyOut: String, valueOut: String): Unit
  }

  /**
   * This trait is supposed to be implemented by the application to process and transform the data
   * in a certain way. Each node runs an instance of the mapper class.
   *
   * It is possible to have state in the mapper implementations. The [[Emitter]] implementations usually store
   * some state itself in each mapper.
   *
   * See [[cse2520.mapreduce.wordcount.WordCountMapper]] for an example.
   *
   * A different mapper implementation can be used by changing the <code>map-reduce.nodes.mapper-class</code>
   * property in <code>application.conf</code> file.
   **/
  trait Mapper {
    def map(mapEmitter: Emitter, key: String, value: String): Try[Unit]
  }

  /**
   * This trait is used to aggregate and combine the values emitted by mappers. The Map-Reduce framework makes
   * sure that keys generated by different mappers end-up in the same reducer by using a partitioner. Also, the
   * keys are sorted - lexicographic order for this application.
   *
   * Like the mappers, each node runs a instance of the reducer class.
   *
   * See [[cse2520.mapreduce.wordcount.WordCountMapper]] for an example.
   *
   * A different reducer implementation can be used by changing the <code>map-reduce.nodes.reducer-class</code>
   * property in <code>application.conf</code> file.
   */
  trait Reducer {
    def reduce(reduceEmitter: Emitter, key: String, values: Iterable[String]): Try[Unit]
  }

  object Partitioner {
    def hashPartitions(count: Int): String => Int =
      key => {
        if (key == null) 0
        else Math.abs(key.hashCode) % count
      }

    val singlePartitioner: Partitioner = new Partitioner(1, _ => 0)

    def apply(): Partitioner = singlePartitioner

    def apply(count: Int): Partitioner = {
      if (count <= 0) throw new IllegalArgumentException("Partition count should be greater than 0")
      else if (count == 1) singlePartitioner
      else new Partitioner(count, hashPartitions(count))
    }
  }

  final class Partitioner(val count: Int, private val partitioner: String => Int) {
    def partition(in: String): Int = partitioner(in)
  }

  /**
   * Emulates the shared file system (such as GFS used by Hadoop).
   */
  trait FileSystem {

    /**
     * Returns the names of input sets to be processed by the mappers. In this simplified system, the input is assumed
     * to be split beforehand.
     */
    def getInputSets: List[String]

    /**
     * Reads the input set lazily, evaluating a line on each iteration.
     *
     * @param inFile name of input file
     * @return stream of lines from the content of the input file
     */
    def readInputSet(inFile: String): LazyList[String]

    /**
     * Writes the output generated by reducers to the file system.
     *
     * @param outFile name of output file
     * @param values the lines to be written
     */
    def writeOutputSet(outFile: String, values: List[String]): Unit

    def writeLocalSet(outFile: String, values: Seq[(String, String)]): Unit

    def readRemoteSet(inFile: String): LazyList[(String, String)]
  }

  class PartitioningBufferedEmitter(val partitioner: Partitioner) extends Emitter {

    // Array indexes are partitions. On each index, the emitted items are prepended to the existing list.
    // Each partition starts with an empty list.
    private val buffer: Array[BufferedEmitter] = Array.fill(partitioner.count) {
      new BufferedEmitter()
    }

    override def emit(keyOut: String, valueOut: String): Unit = {
      val partitionId = partitioner.partition(keyOut)

      buffer(partitionId).emit(keyOut, valueOut)
    }

    def getData(partitionId: Int): List[(String, String)] = buffer(partitionId).getData
  }

  class BufferedEmitter extends Emitter {

    private var buffer = List[(String, String)]()

    override def emit(keyOut: String, valueOut: String): Unit =
      buffer = buffer :+ (keyOut -> valueOut)

    def getData: List[(String, String)] = buffer
  }

  // Job Configuration Classes
  case class Directories(inputs: String, work: String, outputs: String)

  case class Nodes(mapperCount: Int, mapperClass: Class[Mapper], reducerCount: Int, reducerClass: Class[Reducer])

  case class JobConfig(dirs: Directories, nodes: Nodes)
}
